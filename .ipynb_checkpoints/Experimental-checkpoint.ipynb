{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a859a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you see upload widgets below, use them. Otherwise, set file paths in the fallback section.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ac86d0f78842378c8eacf8f41464b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv,.xlsx,.xls', description='Upload data1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91645e03f68040cb8f664dcf3b2efb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv,.xlsx,.xls', description='Upload data2')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6e9386796e4ce9b59494519547bd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Output', options=('CSV', 'Excel'), value='CSV')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1643768b15664cbab9a54fbd725d9429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Run Pipeline', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d57a3afd4d640ef8e912eea1a384f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AI/ML Data Format Alignment Pipeline for Jupyter\n",
    "# Author: You\n",
    "# Purpose: Map data2 into data1's schema and export outputdata as CSV or Excel\n",
    "\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    IPYW_AVAILABLE = True\n",
    "except Exception:\n",
    "    IPYW_AVAILABLE = False\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "\n",
    "def _normalize_colname(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[\\s\\-_]+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9 %]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _infer_dtype_series(s: pd.Series) -> str:\n",
    "    # Try boolean\n",
    "    unique_lower = set(str(x).strip().lower() for x in s.dropna().unique()[:100])\n",
    "    bool_set = {\"true\",\"false\",\"yes\",\"no\",\"y\",\"n\",\"0\",\"1\"}\n",
    "    if unique_lower and unique_lower.issubset(bool_set):\n",
    "        return \"boolean\"\n",
    "\n",
    "    # Try datetime\n",
    "    try:\n",
    "        pd.to_datetime(s.dropna().sample(min(len(s.dropna()), 200), random_state=0), errors=\"raise\", infer_datetime_format=True)\n",
    "        return \"datetime\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try integer\n",
    "    try:\n",
    "        x = pd.to_numeric(s.dropna(), errors=\"raise\")\n",
    "        if np.all(np.mod(x, 1) == 0):\n",
    "            return \"integer\"\n",
    "        return \"float\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return \"string\"\n",
    "\n",
    "def infer_schema(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    schema = {}\n",
    "    for c in df.columns:\n",
    "        schema[c] = _infer_dtype_series(df[c])\n",
    "    return schema\n",
    "\n",
    "def _coerce_to_dtype(series: pd.Series, target: str) -> Tuple[pd.Series, float]:\n",
    "    s = series.copy()\n",
    "    success_ratio = 1.0\n",
    "\n",
    "    if target == \"string\":\n",
    "        s = s.astype(str)\n",
    "        return s, success_ratio\n",
    "\n",
    "    if target == \"boolean\":\n",
    "        mapping = {\n",
    "            \"true\": True, \"false\": False,\n",
    "            \"yes\": True, \"no\": False,\n",
    "            \"y\": True, \"n\": False,\n",
    "            \"1\": True, \"0\": False,\n",
    "            \"True\": True, \"False\": False\n",
    "        }\n",
    "        original = s.copy()\n",
    "        s = s.astype(str).str.strip()\n",
    "        s = s.map(lambda x: mapping.get(x, np.nan))\n",
    "        success_ratio = 1.0 - (s.isna() & ~original.isna()).mean()\n",
    "        return s, success_ratio\n",
    "\n",
    "    if target in (\"integer\", \"float\"):\n",
    "        coerced = pd.to_numeric(s, errors=\"coerce\")\n",
    "        success_ratio = 1.0 - coerced.isna().mean()\n",
    "        if target == \"integer\":\n",
    "            # keep as Int64 to allow NA\n",
    "            s = coerced.round().astype(\"Int64\")\n",
    "        else:\n",
    "            s = coerced.astype(float)\n",
    "        return s, success_ratio\n",
    "\n",
    "    if target == \"datetime\":\n",
    "        coerced = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "        success_ratio = 1.0 - coerced.isna().mean()\n",
    "        return coerced, success_ratio\n",
    "\n",
    "    # default\n",
    "    return s, 1.0\n",
    "\n",
    "def best_column_match(target_col: str, candidates: List[str], threshold: float = 0.75) -> Optional[str]:\n",
    "    # exact normalized match first\n",
    "    norm_target = _normalize_colname(target_col)\n",
    "    norm_map = {_normalize_colname(c): c for c in candidates}\n",
    "    if norm_target in norm_map:\n",
    "        return norm_map[norm_target]\n",
    "    # difflib similarity\n",
    "    best = difflib.get_close_matches(norm_target, list(norm_map.keys()), n=1, cutoff=threshold)\n",
    "    if best:\n",
    "        return norm_map[best[0]]\n",
    "    return None\n",
    "\n",
    "def build_column_mapping(\n",
    "    data1_cols: List[str],\n",
    "    data2_cols: List[str],\n",
    "    cutoff: float = 0.75\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    mapping = {}\n",
    "    unmatched_data2 = set(data2_cols)\n",
    "    # first pass: exact normalized matches\n",
    "    norm_to_data2 = {_normalize_colname(c): c for c in data2_cols}\n",
    "    for c in data1_cols:\n",
    "        norm = _normalize_colname(c)\n",
    "        if norm in norm_to_data2:\n",
    "            mapping[c] = norm_to_data2[norm]\n",
    "            unmatched_data2.discard(norm_to_data2[norm])\n",
    "        else:\n",
    "            mapping[c] = None\n",
    "    # second pass: fuzzy for those still None\n",
    "    for c in data1_cols:\n",
    "        if mapping[c] is None:\n",
    "            match = best_column_match(c, list(unmatched_data2), threshold=cutoff)\n",
    "            if match is not None:\n",
    "                mapping[c] = match\n",
    "                unmatched_data2.discard(match)\n",
    "    return mapping\n",
    "\n",
    "def align_to_schema(\n",
    "    df_data2: pd.DataFrame,\n",
    "    schema_data1: Dict[str, str],\n",
    "    mapping: Dict[str, Optional[str]],\n",
    "    fill_missing_with: Any = np.nan\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    report = {\n",
    "        \"matched_columns\": [],\n",
    "        \"unmatched_data1_columns\": [],\n",
    "        \"unmapped_data2_columns\": [],\n",
    "        \"type_coercion_success\": {},\n",
    "    }\n",
    "\n",
    "    # Track unmapped data2\n",
    "    mapped_targets = set([v for v in mapping.values() if v is not None])\n",
    "    report[\"unmapped_data2_columns\"] = [c for c in df_data2.columns if c not in mapped_targets]\n",
    "\n",
    "    aligned = pd.DataFrame()\n",
    "    for col_data1, dtype in schema_data1.items():\n",
    "        source_col = mapping.get(col_data1, None)\n",
    "        if source_col is not None and source_col in df_data2.columns:\n",
    "            series = df_data2[source_col]\n",
    "            # basic string cleanup if target string\n",
    "            if dtype == \"string\":\n",
    "                series = series.astype(str).str.strip()\n",
    "            coerced, success = _coerce_to_dtype(series, dtype)\n",
    "            aligned[col_data1] = coerced\n",
    "            report[\"matched_columns\"].append((col_data1, source_col, round(success, 4)))\n",
    "            report[\"type_coercion_success\"][col_data1] = round(success, 4)\n",
    "        else:\n",
    "            aligned[col_data1] = fill_missing_with\n",
    "            report[\"unmatched_data1_columns\"].append(col_data1)\n",
    "\n",
    "    # reorder columns to data1 order\n",
    "    aligned = aligned[list(schema_data1.keys())]\n",
    "    return aligned, report\n",
    "\n",
    "def print_report(schema: Dict[str, str], mapping: Dict[str, Optional[str]], report: Dict[str, Any]) -> None:\n",
    "    print(\"Data1 schema\")\n",
    "    print(pd.DataFrame(list(schema.items()), columns=[\"column\", \"dtype\"]).to_string(index=False))\n",
    "    print(\"\\nColumn mapping data1 -> data2\")\n",
    "    mapping_rows = []\n",
    "    for k, v in mapping.items():\n",
    "        mapping_rows.append((k, v if v is not None else \"None\"))\n",
    "    print(pd.DataFrame(mapping_rows, columns=[\"data1_column\", \"data2_source\"]).to_string(index=False))\n",
    "\n",
    "    print(\"\\nMatched columns with coercion success\")\n",
    "    if report[\"matched_columns\"]:\n",
    "        print(pd.DataFrame(report[\"matched_columns\"], columns=[\"data1_column\", \"data2_source\", \"success\"]).to_string(index=False))\n",
    "    else:\n",
    "        print(\"None\")\n",
    "\n",
    "    print(\"\\nUnmatched data1 columns\")\n",
    "    print(report[\"unmatched_data1_columns\"] if report[\"unmatched_data1_columns\"] else \"None\")\n",
    "\n",
    "    print(\"\\nUnmapped data2 columns\")\n",
    "    print(report[\"unmapped_data2_columns\"] if report[\"unmapped_data2_columns\"] else \"None\")\n",
    "\n",
    "    if report.get(\"type_coercion_success\"):\n",
    "        successes = list(report[\"type_coercion_success\"].values())\n",
    "        overall = round(float(np.mean(successes)), 4) if successes else 0.0\n",
    "        print(f\"\\nApproximate overall type coercion success: {overall}\")\n",
    "\n",
    "def _read_any(file_bytes: bytes, filename: str) -> pd.DataFrame:\n",
    "    name = filename.lower()\n",
    "    if name.endswith(\".csv\"):\n",
    "        return pd.read_csv(io.BytesIO(file_bytes))\n",
    "    if name.endswith(\".xlsx\") or name.endswith(\".xls\"):\n",
    "        return pd.read_excel(io.BytesIO(file_bytes))\n",
    "    raise ValueError(\"Unsupported file format. Use .csv or .xlsx\")\n",
    "\n",
    "def _read_path(path: str) -> pd.DataFrame:\n",
    "    if path.lower().endswith(\".csv\"):\n",
    "        return pd.read_csv(path)\n",
    "    if path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        return pd.read_excel(path)\n",
    "    raise ValueError(\"Unsupported file format. Use .csv or .xlsx\")\n",
    "\n",
    "def _write_output(df: pd.DataFrame, out_path: str) -> str:\n",
    "    if out_path.lower().endswith(\".csv\"):\n",
    "        df.to_csv(out_path, index=False)\n",
    "    elif out_path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df.to_excel(out_path, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Output file must be .csv or .xlsx\")\n",
    "    return os.path.abspath(out_path)\n",
    "\n",
    "# -------------------------\n",
    "# UI: either widgets upload or direct paths\n",
    "# -------------------------\n",
    "\n",
    "print(\"If you see upload widgets below, use them. Otherwise, set file paths in the fallback section.\")\n",
    "\n",
    "upload_data1 = None\n",
    "upload_data2 = None\n",
    "out_format = None\n",
    "\n",
    "if IPYW_AVAILABLE:\n",
    "    fu1 = widgets.FileUpload(accept=\".csv,.xlsx,.xls\", multiple=False, description=\"Upload data1\")\n",
    "    fu2 = widgets.FileUpload(accept=\".csv,.xlsx,.xls\", multiple=False, description=\"Upload data2\")\n",
    "    fmt = widgets.Dropdown(options=[\"CSV\", \"Excel\"], value=\"CSV\", description=\"Output\")\n",
    "    run_btn = widgets.Button(description=\"Run Pipeline\", button_style=\"primary\")\n",
    "    out_area = widgets.Output()\n",
    "\n",
    "    def on_run_clicked(_):\n",
    "        with out_area:\n",
    "            out_area.clear_output()\n",
    "            if len(fu1.value) == 0 or len(fu2.value) == 0:\n",
    "                print(\"Please upload both files.\")\n",
    "                return\n",
    "            # read\n",
    "            key1 = list(fu1.value.keys())[0]\n",
    "            key2 = list(fu2.value.keys())[0]\n",
    "            f1 = fu1.value[key1]\n",
    "            f2 = fu2.value[key2]\n",
    "            df1 = _read_any(f1[\"content\"], key1)\n",
    "            df2 = _read_any(f2[\"content\"], key2)\n",
    "\n",
    "            print(\"Files loaded\")\n",
    "            print(f\"data1 shape: {df1.shape}\")\n",
    "            print(f\"data2 shape: {df2.shape}\")\n",
    "\n",
    "            schema = infer_schema(df1)\n",
    "            mapping = build_column_mapping(list(df1.columns), list(df2.columns), cutoff=0.75)\n",
    "            aligned, rep = align_to_schema(df2, schema, mapping, fill_missing_with=np.nan)\n",
    "\n",
    "            print_report(schema, mapping, rep)\n",
    "\n",
    "            # choose output path in working dir\n",
    "            if fmt.value == \"CSV\":\n",
    "                out_path = \"outputdata.csv\"\n",
    "            else:\n",
    "                out_path = \"outputdata.xlsx\"\n",
    "\n",
    "            abs_path = _write_output(aligned, out_path)\n",
    "            print(f\"\\nOutput written to: {abs_path}\")\n",
    "\n",
    "    run_btn.on_click(on_run_clicked)\n",
    "\n",
    "    display(fu1, fu2, fmt, run_btn, out_area)\n",
    "\n",
    "# -------------------------\n",
    "# Fallback: use local file paths if widgets are not available\n",
    "# -------------------------\n",
    "\n",
    "# Uncomment and set these paths if running in a non-widget environment:\n",
    "# data1_path = \"path/to/your/data1.xlsx\"  # or .csv\n",
    "# data2_path = \"path/to/your/data2.csv\"   # or .xlsx\n",
    "# output_path = \"outputdata.csv\"          # or .xlsx\n",
    "\n",
    "# If you use fallback, run this block after setting paths above:\n",
    "# df1 = _read_path(data1_path)\n",
    "# df2 = _read_path(data2_path)\n",
    "# schema = infer_schema(df1)\n",
    "# mapping = build_column_mapping(list(df1.columns), list(df2.columns), cutoff=0.75)\n",
    "# aligned, rep = align_to_schema(df2, schema, mapping, fill_missing_with=np.nan)\n",
    "# print_report(schema, mapping, rep)\n",
    "# abs_path = _write_output(aligned, output_path)\n",
    "# print(f\"\\nOutput written to: {abs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87bc9a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column mapping (data1 -> data2):\n",
      "{'Student_Name': 'Student_Name', 'Student_Surename': 'Student_Surename', 'Student_Category': 'Student_Category', 'Student_Address': 'Student_Address', 'Student_Batch': 'Student_Batch', 'Student_Specilization': 'Student_Specilization', 'Student_Cell_No': 'Student_Cell_No', 'Stu_EmailID': 'Stu_EmailID'}\n",
      "Output file saved as outputdata.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import difflib\n",
    "import numpy as np\n",
    "\n",
    "# Load files\n",
    "data1 = pd.read_excel(\"format.xlsx\")   # or pd.read_csv(\"data1.csv\")\n",
    "data2 = pd.read_excel(\"Infor_file.xlsx\")   # or pd.read_csv(\"data2.csv\")\n",
    "\n",
    "# Use only column names from data1 (schema)\n",
    "schema_columns = list(data1.columns)\n",
    "\n",
    "# Prepare an empty DataFrame with schema\n",
    "output_df = pd.DataFrame(columns=schema_columns)\n",
    "\n",
    "# Try to map each schema column to the closest match in data2\n",
    "column_mapping = {}\n",
    "for col in schema_columns:\n",
    "    best_match = difflib.get_close_matches(col, data2.columns, n=1, cutoff=0.6)\n",
    "    if best_match:\n",
    "        column_mapping[col] = best_match[0]\n",
    "    else:\n",
    "        column_mapping[col] = None\n",
    "\n",
    "print(\"Column mapping (data1 -> data2):\")\n",
    "print(column_mapping)\n",
    "\n",
    "# Fill the output_df according to mapping\n",
    "for col in schema_columns:\n",
    "    if column_mapping[col] is not None:\n",
    "        output_df[col] = data2[column_mapping[col]]\n",
    "    else:\n",
    "        output_df[col] = np.nan  # blank if not found\n",
    "\n",
    "# Save output file\n",
    "output_df.to_excel(\"outputdata.xlsx\", index=False)\n",
    "print(\"Output file saved as outputdata.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1db5a628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you see upload widgets below, use them. Otherwise, set file paths manually in fallback section.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb05e3c61424eadb310eaf4882a8a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv,.xlsx,.xls', description='Upload data1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17aa14fbe48740c295ff48e366622f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv,.xlsx,.xls', description='Upload data2')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230411bd9ba34e26945362b9ed7b16b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Output', options=('CSV', 'Excel'), value='CSV')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa073833aa89489d81732767321f24d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Run Pipeline', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb01372450874d099fa514172cd9252e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AI/ML Data Format Alignment Pipeline for Jupyter\n",
    "# Author: You\n",
    "# Purpose: User uploads data1 (schema) and data2 (content), program maps data2 into data1's schema\n",
    "#          and generates outputdata.csv or outputdata.xlsx\n",
    "\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    IPYW_AVAILABLE = True\n",
    "except Exception:\n",
    "    IPYW_AVAILABLE = False\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "\n",
    "def _normalize_colname(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[\\s\\-_]+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9 %]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _infer_dtype_series(s: pd.Series) -> str:\n",
    "    unique_lower = set(str(x).strip().lower() for x in s.dropna().unique()[:100])\n",
    "    bool_set = {\"true\",\"false\",\"yes\",\"no\",\"y\",\"n\",\"0\",\"1\"}\n",
    "    if unique_lower and unique_lower.issubset(bool_set):\n",
    "        return \"boolean\"\n",
    "    try:\n",
    "        pd.to_datetime(s.dropna().sample(min(len(s.dropna()), 200), random_state=0), errors=\"raise\", infer_datetime_format=True)\n",
    "        return \"datetime\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        x = pd.to_numeric(s.dropna(), errors=\"raise\")\n",
    "        if np.all(np.mod(x, 1) == 0):\n",
    "            return \"integer\"\n",
    "        return \"float\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"string\"\n",
    "\n",
    "def infer_schema(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    return {c: _infer_dtype_series(df[c]) for c in df.columns}\n",
    "\n",
    "def _coerce_to_dtype(series: pd.Series, target: str) -> Tuple[pd.Series, float]:\n",
    "    s = series.copy()\n",
    "    success_ratio = 1.0\n",
    "    if target == \"string\":\n",
    "        return s.astype(str), success_ratio\n",
    "    if target == \"boolean\":\n",
    "        mapping = {\"true\": True, \"false\": False,\"yes\": True, \"no\": False,\"y\": True, \"n\": False,\"1\": True, \"0\": False}\n",
    "        original = s.copy()\n",
    "        s = s.astype(str).str.strip()\n",
    "        s = s.map(lambda x: mapping.get(x, np.nan))\n",
    "        success_ratio = 1.0 - (s.isna() & ~original.isna()).mean()\n",
    "        return s, success_ratio\n",
    "    if target in (\"integer\", \"float\"):\n",
    "        coerced = pd.to_numeric(s, errors=\"coerce\")\n",
    "        success_ratio = 1.0 - coerced.isna().mean()\n",
    "        if target == \"integer\":\n",
    "            s = coerced.round().astype(\"Int64\")\n",
    "        else:\n",
    "            s = coerced.astype(float)\n",
    "        return s, success_ratio\n",
    "    if target == \"datetime\":\n",
    "        coerced = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "        success_ratio = 1.0 - coerced.isna().mean()\n",
    "        return coerced, success_ratio\n",
    "    return s, 1.0\n",
    "\n",
    "def best_column_match(target_col: str, candidates: List[str], threshold: float = 0.75) -> Optional[str]:\n",
    "    norm_target = _normalize_colname(target_col)\n",
    "    norm_map = {_normalize_colname(c): c for c in candidates}\n",
    "    if norm_target in norm_map:\n",
    "        return norm_map[norm_target]\n",
    "    best = difflib.get_close_matches(norm_target, list(norm_map.keys()), n=1, cutoff=threshold)\n",
    "    if best:\n",
    "        return norm_map[best[0]]\n",
    "    return None\n",
    "\n",
    "def build_column_mapping(data1_cols: List[str], data2_cols: List[str], cutoff: float = 0.75) -> Dict[str, Optional[str]]:\n",
    "    mapping = {}\n",
    "    unmatched_data2 = set(data2_cols)\n",
    "    norm_to_data2 = {_normalize_colname(c): c for c in data2_cols}\n",
    "    for c in data1_cols:\n",
    "        norm = _normalize_colname(c)\n",
    "        if norm in norm_to_data2:\n",
    "            mapping[c] = norm_to_data2[norm]\n",
    "            unmatched_data2.discard(norm_to_data2[norm])\n",
    "        else:\n",
    "            mapping[c] = None\n",
    "    for c in data1_cols:\n",
    "        if mapping[c] is None:\n",
    "            match = best_column_match(c, list(unmatched_data2), threshold=cutoff)\n",
    "            if match is not None:\n",
    "                mapping[c] = match\n",
    "                unmatched_data2.discard(match)\n",
    "    return mapping\n",
    "\n",
    "def align_to_schema(df_data2: pd.DataFrame, schema_data1: Dict[str, str], mapping: Dict[str, Optional[str]], fill_missing_with: Any = np.nan) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    report = {\"matched_columns\": [],\"unmatched_data1_columns\": [],\"unmapped_data2_columns\": [],\"type_coercion_success\": {}}\n",
    "    mapped_targets = set([v for v in mapping.values() if v is not None])\n",
    "    report[\"unmapped_data2_columns\"] = [c for c in df_data2.columns if c not in mapped_targets]\n",
    "    aligned = pd.DataFrame()\n",
    "    for col_data1, dtype in schema_data1.items():\n",
    "        source_col = mapping.get(col_data1, None)\n",
    "        if source_col is not None and source_col in df_data2.columns:\n",
    "            series = df_data2[source_col]\n",
    "            if dtype == \"string\":\n",
    "                series = series.astype(str).str.strip()\n",
    "            coerced, success = _coerce_to_dtype(series, dtype)\n",
    "            aligned[col_data1] = coerced\n",
    "            report[\"matched_columns\"].append((col_data1, source_col, round(success, 4)))\n",
    "            report[\"type_coercion_success\"][col_data1] = round(success, 4)\n",
    "        else:\n",
    "            aligned[col_data1] = fill_missing_with\n",
    "            report[\"unmatched_data1_columns\"].append(col_data1)\n",
    "    aligned = aligned[list(schema_data1.keys())]\n",
    "    return aligned, report\n",
    "\n",
    "def print_report(schema: Dict[str, str], mapping: Dict[str, Optional[str]], report: Dict[str, Any]) -> None:\n",
    "    print(\"Data1 schema\")\n",
    "    print(pd.DataFrame(list(schema.items()), columns=[\"column\", \"dtype\"]).to_string(index=False))\n",
    "    print(\"\\nColumn mapping data1 -> data2\")\n",
    "    print(pd.DataFrame([(k, v if v is not None else \"None\") for k,v in mapping.items()], columns=[\"data1_column\",\"data2_source\"]).to_string(index=False))\n",
    "    print(\"\\nMatched columns with coercion success\")\n",
    "    if report[\"matched_columns\"]:\n",
    "        print(pd.DataFrame(report[\"matched_columns\"], columns=[\"data1_column\",\"data2_source\",\"success\"]).to_string(index=False))\n",
    "    else:\n",
    "        print(\"None\")\n",
    "    print(\"\\nUnmatched data1 columns\")\n",
    "    print(report[\"unmatched_data1_columns\"] if report[\"unmatched_data1_columns\"] else \"None\")\n",
    "    print(\"\\nUnmapped data2 columns\")\n",
    "    print(report[\"unmapped_data2_columns\"] if report[\"unmapped_data2_columns\"] else \"None\")\n",
    "    if report.get(\"type_coercion_success\"):\n",
    "        successes = list(report[\"type_coercion_success\"].values())\n",
    "        overall = round(float(np.mean(successes)), 4) if successes else 0.0\n",
    "        print(f\"\\nApproximate overall type coercion success: {overall}\")\n",
    "\n",
    "def _read_any(file_bytes: bytes, filename: str) -> pd.DataFrame:\n",
    "    if filename.lower().endswith(\".csv\"):\n",
    "        return pd.read_csv(io.BytesIO(file_bytes))\n",
    "    if filename.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        return pd.read_excel(io.BytesIO(file_bytes))\n",
    "    raise ValueError(\"Unsupported file format. Use .csv or .xlsx\")\n",
    "\n",
    "def _write_output(df: pd.DataFrame, out_path: str) -> str:\n",
    "    if out_path.lower().endswith(\".csv\"):\n",
    "        df.to_csv(out_path, index=False)\n",
    "    elif out_path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df.to_excel(out_path, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Output file must be .csv or .xlsx\")\n",
    "    return os.path.abspath(out_path)\n",
    "\n",
    "# -------------------------\n",
    "# Jupyter UI\n",
    "# -------------------------\n",
    "\n",
    "print(\"If you see upload widgets below, use them. Otherwise, set file paths manually in fallback section.\")\n",
    "\n",
    "if IPYW_AVAILABLE:\n",
    "    fu1 = widgets.FileUpload(accept=\".csv,.xlsx,.xls\", multiple=False, description=\"Upload data1\")\n",
    "    fu2 = widgets.FileUpload(accept=\".csv,.xlsx,.xls\", multiple=False, description=\"Upload data2\")\n",
    "    fmt = widgets.Dropdown(options=[\"CSV\", \"Excel\"], value=\"CSV\", description=\"Output\")\n",
    "    run_btn = widgets.Button(description=\"Run Pipeline\", button_style=\"primary\")\n",
    "    out_area = widgets.Output()\n",
    "\n",
    "    def on_run_clicked(_):\n",
    "        with out_area:\n",
    "            out_area.clear_output()\n",
    "            if len(fu1.value) == 0 or len(fu2.value) == 0:\n",
    "                print(\"Please upload both files.\")\n",
    "                return\n",
    "            key1 = list(fu1.value.keys())[0]\n",
    "            key2 = list(fu2.value.keys())[0]\n",
    "            f1 = fu1.value[key1]\n",
    "            f2 = fu2.value[key2]\n",
    "            df1 = _read_any(f1[\"content\"], key1)\n",
    "            df2 = _read_any(f2[\"content\"], key2)\n",
    "            print(\"Files loaded\")\n",
    "            print(f\"data1 shape: {df1.shape}\")\n",
    "            print(f\"data2 shape: {df2.shape}\")\n",
    "            schema = infer_schema(df1)\n",
    "            mapping = build_column_mapping(list(df1.columns), list(df2.columns), cutoff=0.75)\n",
    "            aligned, rep = align_to_schema(df2, schema, mapping, fill_missing_with=np.nan)\n",
    "            print_report(schema, mapping, rep)\n",
    "            out_path = \"outputdata.csv\" if fmt.value==\"CSV\" else \"outputdata.xlsx\"\n",
    "            abs_path = _write_output(aligned, out_path)\n",
    "            print(f\"\\nOutput written to: {abs_path}\")\n",
    "\n",
    "    run_btn.on_click(on_run_clicked)\n",
    "    display(fu1, fu2, fmt, run_btn, out_area)\n",
    "\n",
    "# -------------------------\n",
    "# Fallback (no widgets)\n",
    "# -------------------------\n",
    "# Uncomment and set paths if not using widgets\n",
    "# data1_path = \"data1.xlsx\"\n",
    "# data2_path = \"data2.xlsx\"\n",
    "# output_path = \"outputdata.csv\"\n",
    "# df1 = pd.read_excel(data1_path)\n",
    "# df2 = pd.read_excel(data2_path)\n",
    "# schema = infer_schema(df1)\n",
    "# mapping = build_column_mapping(list(df1.columns), list(df2.columns), cutoff=0.75)\n",
    "# aligned, rep = align_to_schema(df2, schema, mapping, fill_missing_with=np.nan)\n",
    "# print_report(schema, mapping, rep)\n",
    "# abs_path = _write_output(aligned, output_path)\n",
    "# print(f\"\\nOutput written to: {abs_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf60dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2688f213",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3a880c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI/ML Data Format Alignment Pipeline (Simplified & Strict Schema Based)\n",
    "# Author: You\n",
    "# Purpose: \n",
    "#   - User uploads data1 (schema) and data2 (content)\n",
    "#   - Program maps data2 into data1's schema\n",
    "#   - Generates outputdata.csv or outputdata.xlsx\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import difflib\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "\n",
    "def _normalize_colname(s: str) -> str:\n",
    "    \"\"\"Lowercase + strip + remove special chars for fuzzy matching.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"[\\s\\-_]+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9 ]\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def best_column_match(target_col: str, candidates: list, threshold: float = 0.6) -> str | None:\n",
    "    \"\"\"Find closest matching column from candidates for target_col.\"\"\"\n",
    "    norm_target = _normalize_colname(target_col)\n",
    "    norm_map = {_normalize_colname(c): c for c in candidates}\n",
    "    if norm_target in norm_map:\n",
    "        return norm_map[norm_target]\n",
    "    best = difflib.get_close_matches(norm_target, list(norm_map.keys()), n=1, cutoff=threshold)\n",
    "    if best:\n",
    "        return norm_map[best[0]]\n",
    "    return None\n",
    "\n",
    "def build_column_mapping(schema_cols: list, data2_cols: list, cutoff: float = 0.6):\n",
    "    \"\"\"Map schema columns (data1) -> closest data2 columns.\"\"\"\n",
    "    mapping = {}\n",
    "    for col in schema_cols:\n",
    "        mapping[col] = best_column_match(col, data2_cols, cutoff)\n",
    "    return mapping\n",
    "\n",
    "def align_to_schema(df_data2: pd.DataFrame, df_data1: pd.DataFrame):\n",
    "    \"\"\"Rearrange df_data2 content into schema of df_data1.\"\"\"\n",
    "    schema_cols = list(df_data1.columns)\n",
    "    mapping = build_column_mapping(schema_cols, list(df_data2.columns))\n",
    "    \n",
    "    output = pd.DataFrame(columns=schema_cols)\n",
    "    for col in schema_cols:\n",
    "        source_col = mapping.get(col)\n",
    "        if source_col and source_col in df_data2.columns:\n",
    "            output[col] = df_data2[source_col]\n",
    "        else:\n",
    "            output[col] = np.nan  # fill empty if no match\n",
    "    \n",
    "    return output, mapping\n",
    "\n",
    "def _read_any(file_bytes: bytes, filename: str) -> pd.DataFrame:\n",
    "    if filename.lower().endswith(\".csv\"):\n",
    "        return pd.read_csv(io.BytesIO(file_bytes))\n",
    "    if filename.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        return pd.read_excel(io.BytesIO(file_bytes))\n",
    "    raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "def _write_output(df: pd.DataFrame, out_path: str):\n",
    "    if out_path.lower().endswith(\".csv\"):\n",
    "        df.to_csv(out_path, index=False)\n",
    "    elif out_path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df.to_excel(out_path, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Output file must be .csv or .xlsx\")\n",
    "    return os.path.abspath(out_path)\n",
    "\n",
    "# -------------------------\n",
    "# Example Usage (No Widgets)\n",
    "# -------------------------\n",
    "# Replace with your actual files\n",
    "# data1_path = \"data1.xlsx\"\n",
    "# data2_path = \"data2.xlsx\"\n",
    "# output_path = \"outputdata.xlsx\"\n",
    "\n",
    "# df1 = pd.read_excel(data1_path)  # schema\n",
    "# df2 = pd.read_excel(data2_path)  # content\n",
    "# aligned, mapping = align_to_schema(df2, df1)\n",
    "# print(\"Column Mapping:\", mapping)\n",
    "# abs_path = _write_output(aligned, output_path)\n",
    "# print(f\"Output written to: {abs_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9c59d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Preview of Aligned Data (first 5 rows) =====\n",
      "\n",
      "  Student_Name Student_Surename Student_Category             Student_Address  \\\n",
      "0        Mohit        Janbandhu               SC  Patil Nagar, Bavdhan, Pune   \n",
      "\n",
      "  Student_Batch Student_Specilization  Student_Cell_No  \\\n",
      "0       2024-25     Business Analysis       8806818081   \n",
      "\n",
      "                Stu_EmailID  \n",
      "0  mohitjanbandhu@gmail.com  \n",
      "\n",
      "=================================================\n",
      "\n",
      "Do you want to save the aligned data? (yes/no): yes\n",
      "Aligned data saved as 'outputdata.csv' and 'outputdata.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import difflib\n",
    "import numpy as np\n",
    "\n",
    "# ===========================================================\n",
    "# AI/ML Data Format Alignment Pipeline (Simplified & Strict Schema Based)\n",
    "# Author: You\n",
    "# Purpose: \n",
    "#   - User uploads data1 (schema) and data2 (content)\n",
    "#   - Program maps data2 into data1's schema\n",
    "#   - Generates outputdata.csv or outputdata.xlsx\n",
    "# ===========================================================\n",
    "\n",
    "# Load schema (data1) and content (data2)\n",
    "data1 = pd.read_excel(\"format.xlsx\")     # Schema file (defines column names)\n",
    "data2 = pd.read_excel(\"Infor_file.xlsx\") # Content file\n",
    "\n",
    "# Extract schema columns\n",
    "schema_columns = list(data1.columns)\n",
    "\n",
    "# Create empty DataFrame with schema columns\n",
    "aligned_data = pd.DataFrame(columns=schema_columns)\n",
    "\n",
    "# Map columns from data2 to schema using fuzzy matching\n",
    "for col in schema_columns:\n",
    "    match = difflib.get_close_matches(col, data2.columns, n=1, cutoff=0.6)\n",
    "    if match:\n",
    "        aligned_data[col] = data2[match[0]]\n",
    "    else:\n",
    "        aligned_data[col] = np.nan  # Fill missing schema columns with NaN\n",
    "\n",
    "# ===========================================================\n",
    "# PREVIEW STEP\n",
    "# ===========================================================\n",
    "print(\"\\n===== Preview of Aligned Data (first 5 rows) =====\\n\")\n",
    "print(aligned_data.head())\n",
    "print(\"\\n=================================================\\n\")\n",
    "user_input = input(\"Do you want to save the aligned data? (yes/no): \").strip().lower()\n",
    "\n",
    "# ===========================================================\n",
    "# SAVE STEP\n",
    "# ===========================================================\n",
    "if user_input in [\"yes\", \"y\"]:\n",
    "    # Save as both CSV and Excel\n",
    "    aligned_data.to_csv(\"outputdata.csv\", index=False)\n",
    "    aligned_data.to_excel(\"outputdata.xlsx\", index=False)\n",
    "    print(\"Aligned data saved as 'outputdata.csv' and 'outputdata.xlsx'\")\n",
    "else:\n",
    "    print(\"Save cancelled. Data not written to file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfa6a4",
   "metadata": {},
   "source": [
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1956819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28769ed754a84e8d9f2d6ec1cd1a30c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv,.xlsx,.xls', description='Upload data1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee41db6a453451f9a03ccd97df1ef9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv,.xlsx,.xls', description='Upload data2')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b20389ee62241309996714cbc99f8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Output', options=('CSV', 'Excel'), value='CSV')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0476778e764e3f95230bb2f7e376cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Run Pipeline', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b06c4c331ed476da5c8f2491beda250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "import difflib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "\n",
    "def align_data(df_schema, df_data):\n",
    "    schema_columns = list(df_schema.columns)\n",
    "    aligned = pd.DataFrame(columns=schema_columns)\n",
    "    mapping = {}\n",
    "    \n",
    "    for col in schema_columns:\n",
    "        match = difflib.get_close_matches(col, df_data.columns, n=1, cutoff=0.6)\n",
    "        if match:\n",
    "            aligned[col] = df_data[match[0]]\n",
    "            mapping[col] = match[0]\n",
    "        else:\n",
    "            aligned[col] = np.nan\n",
    "            mapping[col] = None\n",
    "    return aligned, mapping\n",
    "\n",
    "\n",
    "def preview_and_download(aligned, mapping, fmt):\n",
    "    # Show mapping table\n",
    "    print(\"\\n===== Column Mapping (data1 -> data2) =====\")\n",
    "    for k, v in mapping.items():\n",
    "        print(f\"{k:25s} <-- {v if v else 'None'}\")\n",
    "\n",
    "    # Show preview of aligned data\n",
    "    print(\"\\n===== Preview (first 5 rows) =====\")\n",
    "    display(aligned.head())\n",
    "\n",
    "    # Save output\n",
    "    out_path = \"outputdata.csv\" if fmt == \"CSV\" else \"outputdata.xlsx\"\n",
    "    if fmt == \"CSV\":\n",
    "        aligned.to_csv(out_path, index=False)\n",
    "    else:\n",
    "        aligned.to_excel(out_path, index=False)\n",
    "\n",
    "    # Download link\n",
    "    with open(out_path, \"rb\") as f:\n",
    "        btn = widgets.FileDownload(data=f.read(), \n",
    "                                   filename=out_path, \n",
    "                                   description=f\" Download {out_path}\")\n",
    "    display(btn)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Jupyter UI\n",
    "# -------------------------\n",
    "\n",
    "fu1 = widgets.FileUpload(accept=\".csv,.xlsx,.xls\", multiple=False, description=\"Upload data1\")\n",
    "fu2 = widgets.FileUpload(accept=\".csv,.xlsx,.xls\", multiple=False, description=\"Upload data2\")\n",
    "fmt = widgets.Dropdown(options=[\"CSV\", \"Excel\"], value=\"CSV\", description=\"Output\")\n",
    "run_btn = widgets.Button(description=\"Run Pipeline\", button_style=\"primary\")\n",
    "out_area = widgets.Output()\n",
    "\n",
    "def on_run_clicked(_):\n",
    "    with out_area:\n",
    "        out_area.clear_output()\n",
    "        if len(fu1.value) == 0 or len(fu2.value) == 0:\n",
    "            print(\"Please upload both files.\")\n",
    "            return\n",
    "        \n",
    "        # Read uploaded files\n",
    "        key1 = list(fu1.value.keys())[0]\n",
    "        key2 = list(fu2.value.keys())[0]\n",
    "        f1 = fu1.value[key1]\n",
    "        f2 = fu2.value[key2]\n",
    "\n",
    "        # Auto-detect format\n",
    "        def read_any(file_bytes, filename):\n",
    "            if filename.lower().endswith(\".csv\"):\n",
    "                return pd.read_csv(io.BytesIO(file_bytes))\n",
    "            else:\n",
    "                return pd.read_excel(io.BytesIO(file_bytes))\n",
    "        \n",
    "        df1 = read_any(f1[\"content\"], key1)\n",
    "        df2 = read_any(f2[\"content\"], key2)\n",
    "\n",
    "        print(\"Files loaded\")\n",
    "        print(f\"data1 (schema) shape: {df1.shape}\")\n",
    "        print(f\"data2 (content) shape: {df2.shape}\")\n",
    "\n",
    "        # Align data\n",
    "        aligned, mapping = align_data(df1, df2)\n",
    "        preview_and_download(aligned, mapping, fmt.value)\n",
    "\n",
    "run_btn.on_click(on_run_clicked)\n",
    "display(fu1, fu2, fmt, run_btn, out_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "069c90be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you see upload widgets below, use them. Otherwise, set file paths in the fallback section.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8592eba966d543a0bd58e50ec32974c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv,.xlsx,.xls', description='Upload data1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250948653e1e4cf28f1d8e5569dfd920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv,.xlsx,.xls', description='Upload data2')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40736e21c954886b1c225eface5c98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Output', options=('CSV', 'Excel'), value='CSV')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9efb182a88e40cbbdf6890e08b0cc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Run Pipeline', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4159e4d4a97465db91eef83d95c1bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AI/ML Data Format Alignment Pipeline for Jupyter\n",
    "# Author: You\n",
    "# Purpose: Map data2 into data1's schema and export outputdata as CSV or Excel\n",
    "\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    IPYW_AVAILABLE = True\n",
    "except Exception:\n",
    "    IPYW_AVAILABLE = False\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "\n",
    "def _normalize_colname(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[\\s\\-_]+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9 %]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _infer_dtype_series(s: pd.Series) -> str:\n",
    "    # Try boolean\n",
    "    unique_lower = set(str(x).strip().lower() for x in s.dropna().unique()[:100])\n",
    "    bool_set = {\"true\",\"false\",\"yes\",\"no\",\"y\",\"n\",\"0\",\"1\"}\n",
    "    if unique_lower and unique_lower.issubset(bool_set):\n",
    "        return \"boolean\"\n",
    "\n",
    "    # Try datetime\n",
    "    try:\n",
    "        pd.to_datetime(s.dropna().sample(min(len(s.dropna()), 200), random_state=0), errors=\"raise\", infer_datetime_format=True)\n",
    "        return \"datetime\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try integer\n",
    "    try:\n",
    "        x = pd.to_numeric(s.dropna(), errors=\"raise\")\n",
    "        if np.all(np.mod(x, 1) == 0):\n",
    "            return \"integer\"\n",
    "        return \"float\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return \"string\"\n",
    "\n",
    "def infer_schema(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    schema = {}\n",
    "    for c in df.columns:\n",
    "        schema[c] = _infer_dtype_series(df[c])\n",
    "    return schema\n",
    "\n",
    "def _coerce_to_dtype(series: pd.Series, target: str) -> Tuple[pd.Series, float]:\n",
    "    s = series.copy()\n",
    "    success_ratio = 1.0\n",
    "\n",
    "    if target == \"string\":\n",
    "        s = s.astype(str)\n",
    "        return s, success_ratio\n",
    "\n",
    "    if target == \"boolean\":\n",
    "        mapping = {\n",
    "            \"true\": True, \"false\": False,\n",
    "            \"yes\": True, \"no\": False,\n",
    "            \"y\": True, \"n\": False,\n",
    "            \"1\": True, \"0\": False,\n",
    "            \"True\": True, \"False\": False\n",
    "        }\n",
    "        original = s.copy()\n",
    "        s = s.astype(str).str.strip()\n",
    "        s = s.map(lambda x: mapping.get(x, np.nan))\n",
    "        success_ratio = 1.0 - (s.isna() & ~original.isna()).mean()\n",
    "        return s, success_ratio\n",
    "\n",
    "    if target in (\"integer\", \"float\"):\n",
    "        coerced = pd.to_numeric(s, errors=\"coerce\")\n",
    "        success_ratio = 1.0 - coerced.isna().mean()\n",
    "        if target == \"integer\":\n",
    "            # keep as Int64 to allow NA\n",
    "            s = coerced.round().astype(\"Int64\")\n",
    "        else:\n",
    "            s = coerced.astype(float)\n",
    "        return s, success_ratio\n",
    "\n",
    "    if target == \"datetime\":\n",
    "        coerced = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "        success_ratio = 1.0 - coerced.isna().mean()\n",
    "        return coerced, success_ratio\n",
    "\n",
    "    # default\n",
    "    return s, 1.0\n",
    "\n",
    "def best_column_match(target_col: str, candidates: List[str], threshold: float = 0.75) -> Optional[str]:\n",
    "    # exact normalized match first\n",
    "    norm_target = _normalize_colname(target_col)\n",
    "    norm_map = {_normalize_colname(c): c for c in candidates}\n",
    "    if norm_target in norm_map:\n",
    "        return norm_map[norm_target]\n",
    "    # difflib similarity\n",
    "    best = difflib.get_close_matches(norm_target, list(norm_map.keys()), n=1, cutoff=threshold)\n",
    "    if best:\n",
    "        return norm_map[best[0]]\n",
    "    return None\n",
    "\n",
    "def build_column_mapping(\n",
    "    data1_cols: List[str],\n",
    "    data2_cols: List[str],\n",
    "    cutoff: float = 0.75\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    mapping = {}\n",
    "    unmatched_data2 = set(data2_cols)\n",
    "    # first pass: exact normalized matches\n",
    "    norm_to_data2 = {_normalize_colname(c): c for c in data2_cols}\n",
    "    for c in data1_cols:\n",
    "        norm = _normalize_colname(c)\n",
    "        if norm in norm_to_data2:\n",
    "            mapping[c] = norm_to_data2[norm]\n",
    "            unmatched_data2.discard(norm_to_data2[norm])\n",
    "        else:\n",
    "            mapping[c] = None\n",
    "    # second pass: fuzzy for those still None\n",
    "    for c in data1_cols:\n",
    "        if mapping[c] is None:\n",
    "            match = best_column_match(c, list(unmatched_data2), threshold=cutoff)\n",
    "            if match is not None:\n",
    "                mapping[c] = match\n",
    "                unmatched_data2.discard(match)\n",
    "    return mapping\n",
    "\n",
    "def align_to_schema(\n",
    "    df_data2: pd.DataFrame,\n",
    "    schema_data1: Dict[str, str],\n",
    "    mapping: Dict[str, Optional[str]],\n",
    "    fill_missing_with: Any = np.nan\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    report = {\n",
    "        \"matched_columns\": [],\n",
    "        \"unmatched_data1_columns\": [],\n",
    "        \"unmapped_data2_columns\": [],\n",
    "        \"type_coercion_success\": {},\n",
    "    }\n",
    "\n",
    "    # Track unmapped data2\n",
    "    mapped_targets = set([v for v in mapping.values() if v is not None])\n",
    "    report[\"unmapped_data2_columns\"] = [c for c in df_data2.columns if c not in mapped_targets]\n",
    "\n",
    "    aligned = pd.DataFrame()\n",
    "    for col_data1, dtype in schema_data1.items():\n",
    "        source_col = mapping.get(col_data1, None)\n",
    "        if source_col is not None and source_col in df_data2.columns:\n",
    "            series = df_data2[source_col]\n",
    "            # basic string cleanup if target string\n",
    "            if dtype == \"string\":\n",
    "                series = series.astype(str).str.strip()\n",
    "            coerced, success = _coerce_to_dtype(series, dtype)\n",
    "            aligned[col_data1] = coerced\n",
    "            report[\"matched_columns\"].append((col_data1, source_col, round(success, 4)))\n",
    "            report[\"type_coercion_success\"][col_data1] = round(success, 4)\n",
    "        else:\n",
    "            aligned[col_data1] = fill_missing_with\n",
    "            report[\"unmatched_data1_columns\"].append(col_data1)\n",
    "\n",
    "    # reorder columns to data1 order\n",
    "    aligned = aligned[list(schema_data1.keys())]\n",
    "    return aligned, report\n",
    "\n",
    "def print_report(schema: Dict[str, str], mapping: Dict[str, Optional[str]], report: Dict[str, Any]) -> None:\n",
    "    print(\"Data1 schema\")\n",
    "    print(pd.DataFrame(list(schema.items()), columns=[\"column\", \"dtype\"]).to_string(index=False))\n",
    "    print(\"\\nColumn mapping data1 -> data2\")\n",
    "    mapping_rows = []\n",
    "    for k, v in mapping.items():\n",
    "        mapping_rows.append((k, v if v is not None else \"None\"))\n",
    "    print(pd.DataFrame(mapping_rows, columns=[\"data1_column\", \"data2_source\"]).to_string(index=False))\n",
    "\n",
    "    print(\"\\nMatched columns with coercion success\")\n",
    "    if report[\"matched_columns\"]:\n",
    "        print(pd.DataFrame(report[\"matched_columns\"], columns=[\"data1_column\", \"data2_source\", \"success\"]).to_string(index=False))\n",
    "    else:\n",
    "        print(\"None\")\n",
    "\n",
    "    print(\"\\nUnmatched data1 columns\")\n",
    "    print(report[\"unmatched_data1_columns\"] if report[\"unmatched_data1_columns\"] else \"None\")\n",
    "\n",
    "    print(\"\\nUnmapped data2 columns\")\n",
    "    print(report[\"unmapped_data2_columns\"] if report[\"unmapped_data2_columns\"] else \"None\")\n",
    "\n",
    "    if report.get(\"type_coercion_success\"):\n",
    "        successes = list(report[\"type_coercion_success\"].values())\n",
    "        overall = round(float(np.mean(successes)), 4) if successes else 0.0\n",
    "        print(f\"\\nApproximate overall type coercion success: {overall}\")\n",
    "\n",
    "def _read_any(file_bytes: bytes, filename: str) -> pd.DataFrame:\n",
    "    name = filename.lower()\n",
    "    if name.endswith(\".csv\"):\n",
    "        return pd.read_csv(io.BytesIO(file_bytes))\n",
    "    if name.endswith(\".xlsx\") or name.endswith(\".xls\"):\n",
    "        return pd.read_excel(io.BytesIO(file_bytes))\n",
    "    raise ValueError(\"Unsupported file format. Use .csv or .xlsx\")\n",
    "\n",
    "def _read_path(path: str) -> pd.DataFrame:\n",
    "    if path.lower().endswith(\".csv\"):\n",
    "        return pd.read_csv(path)\n",
    "    if path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        return pd.read_excel(path)\n",
    "    raise ValueError(\"Unsupported file format. Use .csv or .xlsx\")\n",
    "\n",
    "def _write_output(df: pd.DataFrame, out_path: str) -> str:\n",
    "    if out_path.lower().endswith(\".csv\"):\n",
    "        df.to_csv(out_path, index=False)\n",
    "    elif out_path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df.to_excel(out_path, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Output file must be .csv or .xlsx\")\n",
    "    return os.path.abspath(out_path)\n",
    "\n",
    "# -------------------------\n",
    "# UI: either widgets upload or direct paths\n",
    "# -------------------------\n",
    "\n",
    "print(\"If you see upload widgets below, use them. Otherwise, set file paths in the fallback section.\")\n",
    "\n",
    "upload_data1 = None\n",
    "upload_data2 = None\n",
    "out_format = None\n",
    "\n",
    "if IPYW_AVAILABLE:\n",
    "    fu1 = widgets.FileUpload(accept=\".csv,.xlsx,.xls\", multiple=False, description=\"Upload data1\")\n",
    "    fu2 = widgets.FileUpload(accept=\".csv,.xlsx,.xls\", multiple=False, description=\"Upload data2\")\n",
    "    fmt = widgets.Dropdown(options=[\"CSV\", \"Excel\"], value=\"CSV\", description=\"Output\")\n",
    "    run_btn = widgets.Button(description=\"Run Pipeline\", button_style=\"primary\")\n",
    "    out_area = widgets.Output()\n",
    "\n",
    "    def on_run_clicked(_):\n",
    "        with out_area:\n",
    "            out_area.clear_output()\n",
    "            if len(fu1.value) == 0 or len(fu2.value) == 0:\n",
    "                print(\"Please upload both files.\")\n",
    "                return\n",
    "            # read\n",
    "            key1 = list(fu1.value.keys())[0]\n",
    "            key2 = list(fu2.value.keys())[0]\n",
    "            f1 = fu1.value[key1]\n",
    "            f2 = fu2.value[key2]\n",
    "            df1 = _read_any(f1[\"content\"], key1)\n",
    "            df2 = _read_any(f2[\"content\"], key2)\n",
    "\n",
    "            print(\"Files loaded\")\n",
    "            print(f\"data1 shape: {df1.shape}\")\n",
    "            print(f\"data2 shape: {df2.shape}\")\n",
    "\n",
    "            schema = infer_schema(df1)\n",
    "            mapping = build_column_mapping(list(df1.columns), list(df2.columns), cutoff=0.75)\n",
    "            aligned, rep = align_to_schema(df2, schema, mapping, fill_missing_with=np.nan)\n",
    "\n",
    "            print_report(schema, mapping, rep)\n",
    "\n",
    "            # choose output path in working dir\n",
    "            if fmt.value == \"CSV\":\n",
    "                out_path = \"outputdata.csv\"\n",
    "            else:\n",
    "                out_path = \"outputdata.xlsx\"\n",
    "\n",
    "            abs_path = _write_output(aligned, out_path)\n",
    "            print(f\"\\nOutput written to: {abs_path}\")\n",
    "\n",
    "    run_btn.on_click(on_run_clicked)\n",
    "\n",
    "    display(fu1, fu2, fmt, run_btn, out_area)\n",
    "\n",
    "# -------------------------\n",
    "# Fallback: use local file paths if widgets are not available\n",
    "# -------------------------\n",
    "\n",
    "# Uncomment and set these paths if running in a non-widget environment:\n",
    "# data1_path = \"path/to/your/data1.xlsx\"  # or .csv\n",
    "# data2_path = \"path/to/your/data2.csv\"   # or .xlsx\n",
    "# output_path = \"outputdata.csv\"          # or .xlsx\n",
    "\n",
    "# If you use fallback, run this block after setting paths above:\n",
    "# df1 = _read_path(data1_path)\n",
    "# df2 = _read_path(data2_path)\n",
    "# schema = infer_schema(df1)\n",
    "# mapping = build_column_mapping(list(df1.columns), list(df2.columns), cutoff=0.75)\n",
    "# aligned, rep = align_to_schema(df2, schema, mapping, fill_missing_with=np.nan)\n",
    "# print_report(schema, mapping, rep)\n",
    "# abs_path = _write_output(aligned, output_path)\n",
    "# print(f\"\\nOutput written to: {abs_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de4106",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88732a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922faa5c01f04059848888be81fc1819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv,.xlsx,.xls', description='Upload data1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fb6168b2c143ebbac1f05f69305801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv,.xlsx,.xls', description='Upload data2')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acab853f50b74a9f829db6272e40adae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Output', options=('CSV', 'Excel'), value='CSV')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f836a010334126b1509d8746619cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Run Pipeline', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddba46c1c12e448cb744e626f8ca9d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "import difflib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, FileLink\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "\n",
    "def align_data(df_schema, df_data):\n",
    "    schema_columns = list(df_schema.columns)\n",
    "    aligned = pd.DataFrame(columns=schema_columns)\n",
    "    mapping = {}\n",
    "    \n",
    "    for col in schema_columns:\n",
    "        match = difflib.get_close_matches(col, df_data.columns, n=1, cutoff=0.6)\n",
    "        if match:\n",
    "            aligned[col] = df_data[match[0]]\n",
    "            mapping[col] = match[0]\n",
    "        else:\n",
    "            aligned[col] = np.nan\n",
    "            mapping[col] = None\n",
    "    return aligned, mapping\n",
    "\n",
    "\n",
    "def preview_and_download(aligned, mapping, fmt):\n",
    "    # Show mapping table\n",
    "    print(\"\\n===== Column Mapping (data1 -> data2) =====\")\n",
    "    for k, v in mapping.items():\n",
    "        print(f\"{k:25s} <-- {v if v else 'None'}\")\n",
    "\n",
    "    # Show preview of aligned data\n",
    "    print(\"\\n===== Preview (first 5 rows) =====\")\n",
    "    display(aligned.head())\n",
    "\n",
    "    # Save output\n",
    "    out_path = \"outputdata.csv\" if fmt == \"CSV\" else \"outputdata.xlsx\"\n",
    "    if fmt == \"CSV\":\n",
    "        aligned.to_csv(out_path, index=False)\n",
    "    else:\n",
    "        aligned.to_excel(out_path, index=False)\n",
    "\n",
    "    # Provide download link\n",
    "    print(f\"\\nFile saved as {out_path}. Click below to download:\")\n",
    "    display(FileLink(out_path))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Jupyter UI\n",
    "# -------------------------\n",
    "\n",
    "fu1 = widgets.FileUpload(accept=\".csv,.xlsx,.xls\", multiple=False, description=\"Upload data1\")\n",
    "fu2 = widgets.FileUpload(accept=\".csv,.xlsx,.xls\", multiple=False, description=\"Upload data2\")\n",
    "fmt = widgets.Dropdown(options=[\"CSV\", \"Excel\"], value=\"CSV\", description=\"Output\")\n",
    "run_btn = widgets.Button(description=\"Run Pipeline\", button_style=\"primary\")\n",
    "out_area = widgets.Output()\n",
    "\n",
    "def on_run_clicked(_):\n",
    "    with out_area:\n",
    "        out_area.clear_output()\n",
    "        if len(fu1.value) == 0 or len(fu2.value) == 0:\n",
    "            print(\"Please upload both files.\")\n",
    "            return\n",
    "        \n",
    "        # Read uploaded files\n",
    "        key1 = list(fu1.value.keys())[0]\n",
    "        key2 = list(fu2.value.keys())[0]\n",
    "        f1 = fu1.value[key1]\n",
    "        f2 = fu2.value[key2]\n",
    "\n",
    "        # Auto-detect format\n",
    "        def read_any(file_bytes, filename):\n",
    "            if filename.lower().endswith(\".csv\"):\n",
    "                return pd.read_csv(io.BytesIO(file_bytes))\n",
    "            else:\n",
    "                return pd.read_excel(io.BytesIO(file_bytes))\n",
    "        \n",
    "        df1 = read_any(f1[\"content\"], key1)\n",
    "        df2 = read_any(f2[\"content\"], key2)\n",
    "\n",
    "        print(\"Files loaded\")\n",
    "        print(f\"data1 (schema) shape: {df1.shape}\")\n",
    "        print(f\"data2 (content) shape: {df2.shape}\")\n",
    "\n",
    "        # Align data\n",
    "        aligned, mapping = align_data(df1, df2)\n",
    "        preview_and_download(aligned, mapping, fmt.value)\n",
    "\n",
    "run_btn.on_click(on_run_clicked)\n",
    "display(fu1, fu2, fmt, run_btn, out_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2d68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
